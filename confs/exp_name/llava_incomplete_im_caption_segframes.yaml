exp_name: llava_incomplete_im_caption_segframes
device: 'gpu'
num_devices: 1
num_workers: 2

### loading checkpoints, eval, train options
num_epochs: 50
batch_size: 1
lr: 0.000001
weight_decay: 0.0001
lr_scheduler: "cosine"
eval_only: True # if this is set to True, only the val dataset will be run.
tune_lora: True
load_checkpoint: False

lora_model_path: /projectnb/ivc-ml/array/research/robotics/dreamworlds/checkpoints/llava_incomplete_im_caption_segframes/easy-planet-101_85000 # /projectnb/ivc-ml/array/research/robotics/dreamworlds/checkpoints/llava_incomplete_im_caption_polygon_topdown_normal_priv/rural-sky-2_70000 # /projectnb/ivc-ml/array/research/robotics/dreamworlds/checkpoints/llava_incomplete_im_caption_segframes/leafy-wood-91_60000

eval_every: 5000
num_eval_steps: 70
no_shuffle: False

train_dataset_choice: "ProcTHOR_image_caption"
train_dataset_args: {
  split: "train",
  model: "llava",
  model_path: liuhaotian/llava-v1.5-7b,
  use_seg_im: True,
  num_images: 1,
  mode: "train",
  use_panoptic: True,
  include_children: False,
  top_down_pretrained: True, # include this to make sure val sets werent included in the pretraining for top_down.  
  polygon_guided: True,
  im_only: True
}
train_collate_fn: "collate_fn"
 
## define your valtrain dataset, the name must match the class name in datasets/dataloaders.py. This is run during training to help choose best checkpoint
valtrain_dataset_choice:  "ProcTHOR_image_caption"
valtrain_dataset_args: {
  split: "valtrain",
  model: "llava",
  model_path: liuhaotian/llava-v1.5-7b,
  use_seg_im: True,
  num_images: 1,
  mode: "val",
  use_panoptic: True,
  include_children: False,
  top_down_pretrained: True,
  polygon_guided: True,
  im_only: True
}

## define your val dataset, the name must match the class name in datasets/dataloaders.py. This is run only when eval_only is set to True
val_dataset_choice: "ProcTHOR_image_caption"
val_dataset_args: {
  split: "val",
  model: "llava",
  model_path: liuhaotian/llava-v1.5-7b,
  use_seg_im: True,
  num_images: 1,
  mode: "val", 
  use_panoptic: True,
  include_children: False,
  top_down_pretrained: True,
  polygon_guided: True,
  im_only: True
}
val_collate_fn: "collate_fn" 


#define model. The name must match the class name in models/model_interface.py
model_choice: "LlavaModelInterface"
model_init_args: {
  temperature: 0,
  top_p: None,
  num_beams: 1,
  max_new_tokens: 768
}
# the model must output a huggingface output in its forward function.
 
# These are the inputs to the model. These variables must be present in the dictionary output from your dataloader in datasets/dataloaders.py.
model_input_choice: ['input_ids', 'attention_mask', 'pixel_values', 'labels']
 
# these are the losses. Must be defined in models/losses.py
# for each entry, the first variable is the loss name, the second entry is a list of argument inputs to the loss.
# The model outputs can be accessed by model_outputs[key]. The key is the ykey you defined in the dictionary output of the model forward
# The dataloader outputs (say the GT label) can be accessed similary in data_batch[] dictionary

losses: [] 
 
## Metrics [name, torchmetric_class_name_in_eval.py, metric init args, metric update args]:
metrics: [ 
  ['HouseJsonSimilarity', 'HouseJsonSimilarity', [], []],
  ['HouseSemanticSimilarity', 'HouseSemanticSimilarity', [], []]
]
